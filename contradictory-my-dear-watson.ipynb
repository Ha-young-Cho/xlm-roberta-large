{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-10T04:26:51.548335Z","iopub.execute_input":"2023-07-10T04:26:51.549088Z","iopub.status.idle":"2023-07-10T04:26:52.857342Z","shell.execute_reply.started":"2023-07-10T04:26:51.549048Z","shell.execute_reply":"2023-07-10T04:26:52.856463Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/contradictory-my-dear-watson/sample_submission.csv\n/kaggle/input/contradictory-my-dear-watson/train.csv\n/kaggle/input/contradictory-my-dear-watson/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:27:07.363838Z","iopub.execute_input":"2023-07-10T04:27:07.364333Z","iopub.status.idle":"2023-07-10T04:27:07.368528Z","shell.execute_reply.started":"2023-07-10T04:27:07.364301Z","shell.execute_reply":"2023-07-10T04:27:07.367800Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:27:09.545066Z","iopub.execute_input":"2023-07-10T04:27:09.545423Z","iopub.status.idle":"2023-07-10T04:27:50.535802Z","shell.execute_reply.started":"2023-07-10T04:27:09.545396Z","shell.execute_reply":"2023-07-10T04:27:50.534868Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"D0710 04:27:43.077882991      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0710 04:27:43.077927881      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0710 04:27:43.077932212      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0710 04:27:43.077935635      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0710 04:27:43.077938735      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0710 04:27:43.077941966      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0710 04:27:43.077945135      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0710 04:27:43.077948399      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0710 04:27:43.077951549      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0710 04:27:43.077954747      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0710 04:27:43.077957945      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0710 04:27:43.077961075      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0710 04:27:43.077970767      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0710 04:27:43.077973905      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0710 04:27:43.078233936      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\nD0710 04:27:43.083600377      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD0710 04:27:43.083627205      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0710 04:27:43.084091767      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0710 04:27:43.084103709      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0710 04:27:43.084107927      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0710 04:27:43.084111338      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0710 04:27:43.084114954      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0710 04:27:43.084118308      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0710 04:27:43.084125587      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0710 04:27:43.084142315      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0710 04:27:43.084169410      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0710 04:27:43.084184362      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0710 04:27:43.084188073      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0710 04:27:43.084191885      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0710 04:27:43.084198193      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0710 04:27:43.084201830      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0710 04:27:43.084205290      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0710 04:27:43.084209889      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0710 04:27:43.087231840      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0710 04:27:43.107198492     372 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0710 04:27:43.114333100     372 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-07-10T04:27:43.11431546+00:00\", grpc_status:2}\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:27:55.471427Z","iopub.execute_input":"2023-07-10T04:27:55.472560Z","iopub.status.idle":"2023-07-10T04:28:03.590007Z","shell.execute_reply.started":"2023-07-10T04:27:55.472520Z","shell.execute_reply":"2023-07-10T04:28:03.588648Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.8/site-packages (22.0.4)\nCollecting pip\n  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 22.0.4\n    Uninstalling pip-22.0.4:\n      Successfully uninstalled pip-22.0.4\nSuccessfully installed pip-23.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:28:06.500301Z","iopub.execute_input":"2023-07-10T04:28:06.501280Z","iopub.status.idle":"2023-07-10T04:28:22.052524Z","shell.execute_reply.started":"2023-07-10T04:28:06.501246Z","shell.execute_reply":"2023-07-10T04:28:22.051286Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting transformers\n  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.0)\nCollecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nCollecting regex!=2019.12.17 (from transformers)\n  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.28.2)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n  Downloading safetensors-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nCollecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\nInstalling collected packages: tokenizers, safetensors, regex, fsspec, huggingface-hub, transformers\nSuccessfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:28:27.115076Z","iopub.execute_input":"2023-07-10T04:28:27.115986Z","iopub.status.idle":"2023-07-10T04:28:35.740262Z","shell.execute_reply.started":"2023-07-10T04:28:27.115953Z","shell.execute_reply":"2023-07-10T04:28:35.739372Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:28:40.680148Z","iopub.execute_input":"2023-07-10T04:28:40.680498Z","iopub.status.idle":"2023-07-10T04:28:40.829726Z","shell.execute_reply.started":"2023-07-10T04:28:40.680471Z","shell.execute_reply":"2023-07-10T04:28:40.828741Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:28:42.712281Z","iopub.execute_input":"2023-07-10T04:28:42.712680Z","iopub.status.idle":"2023-07-10T04:28:48.876488Z","shell.execute_reply.started":"2023-07-10T04:28:42.712648Z","shell.execute_reply":"2023-07-10T04:28:48.875200Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting sentencepiece\n  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentencepiece\nSuccessfully installed sentencepiece-0.1.99\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"**xlm roberta large**","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaConfig, RobertaModel\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom transformers import XLMRobertaConfig, XLMRobertaModel\n\n# Initializing an XLM-RoBERTa configuration\nconfiguration = XLMRobertaConfig()\n\n# Initializing a model (with random weights) from the configuration\nmodel = XLMRobertaModel(configuration)\n\n# Accessing the model configuration\nconfiguration = model.config\n\nimport tensorflow as tf\n\nmodel_name = 'xlm-roberta-large'\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n\ndef encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('</s>') #sep\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef roberta_encode(hypotheses, premises, tokenizer):\n    num_examples = len(hypotheses)\n  \n    sentence1 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(hypotheses)])\n\n    sentence2 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(premises)])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[s]'])]*sentence1.shape[0]\n    \n    input_word_ids = tf.concat([cls,sentence1,sentence2],axis=-1)\n    \n    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=[input_word_ids.shape[0], 128])\n    \n    type_cls = tf.zeros_like(cls)\n    \n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat(\n        [type_cls, type_s1, type_s2], axis=-1).to_tensor(shape=[input_word_ids.shape[0], 128])\n\n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(shape=[input_word_ids.shape[0], 128]),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}\n\n    return inputs\n\ntrain_input = roberta_encode(train.premise.values, train.hypothesis.values, tokenizer)\n\nmax_len = 128","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:28:54.878711Z","iopub.execute_input":"2023-07-10T04:28:54.879232Z","iopub.status.idle":"2023-07-10T04:29:25.970897Z","shell.execute_reply.started":"2023-07-10T04:28:54.879192Z","shell.execute_reply":"2023-07-10T04:29:25.969691Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 14.5MB/s]\nDownloading (…)lve/main/config.json: 100%|██████████| 616/616 [00:00<00:00, 384kB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"*** layer 3개 + 기본 데이터셋 ***","metadata":{}},{"cell_type":"code","source":"def build_model():\n    roberta_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = roberta_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    x = tf.keras.layers.Dense(128, activation='relu')(embedding[:, 0, :])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(32, activation='relu')(x)\n    output = tf.keras.layers.Dense(3, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nwith strategy.scope():\n    model = build_model()\n    model.summary()\n    \nmodel.fit(train_input, train.label.values, epochs = 18, verbose = 1, batch_size = 256, validation_split = 0.25)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:29:37.167046Z","iopub.execute_input":"2023-07-10T04:29:37.167759Z","iopub.status.idle":"2023-07-10T04:40:23.140092Z","shell.execute_reply.started":"2023-07-10T04:29:37.167722Z","shell.execute_reply":"2023-07-10T04:40:23.138823Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downloading model.safetensors: 100%|██████████| 2.24G/2.24G [00:21<00:00, 105MB/s] \nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFXLMRobertaModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n                                                                                                  \n input_mask (InputLayer)        [(None, 128)]        0           []                               \n                                                                                                  \n input_type_ids (InputLayer)    [(None, 128)]        0           []                               \n                                                                                                  \n tfxlm_roberta_model (TFXLMRobe  TFBaseModelOutputWi  559890432  ['input_word_ids[0][0]',         \n rtaModel)                      thPoolingAndCrossAt               'input_mask[0][0]',             \n                                tentions(last_hidde               'input_type_ids[0][0]']         \n                                n_state=(None, 128,                                               \n                                 1024),                                                           \n                                 pooler_output=(Non                                               \n                                e, 1024),                                                         \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 1024)        0           ['tfxlm_roberta_model[0][0]']    \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 128)          131200      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n batch_normalization (BatchNorm  (None, 128)         512         ['dense[0][0]']                  \n alization)                                                                                       \n                                                                                                  \n dropout_73 (Dropout)           (None, 128)          0           ['batch_normalization[0][0]']    \n                                                                                                  \n dense_1 (Dense)                (None, 64)           8256        ['dropout_73[0][0]']             \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 64)          256         ['dense_1[0][0]']                \n rmalization)                                                                                     \n                                                                                                  \n dropout_74 (Dropout)           (None, 64)           0           ['batch_normalization_1[0][0]']  \n                                                                                                  \n dense_2 (Dense)                (None, 32)           2080        ['dropout_74[0][0]']             \n                                                                                                  \n dense_3 (Dense)                (None, 3)            99          ['dense_2[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 560,032,835\nTrainable params: 560,032,451\nNon-trainable params: 384\n__________________________________________________________________________________________________\nEpoch 1/18\nWARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n2023-07-10 04:31:59.178514: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-07-10 04:32:01.588354: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"36/36 [==============================] - ETA: 0s - loss: 1.3749 - accuracy: 0.3331","output_type":"stream"},{"name":"stderr","text":"2023-07-10 04:35:27.938369: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-07-10 04:35:28.511339: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"36/36 [==============================] - 309s 4s/step - loss: 1.3749 - accuracy: 0.3331 - val_loss: 1.1435 - val_accuracy: 0.3432\nEpoch 2/18\n36/36 [==============================] - 16s 441ms/step - loss: 1.2974 - accuracy: 0.3462 - val_loss: 1.1607 - val_accuracy: 0.3432\nEpoch 3/18\n36/36 [==============================] - 16s 441ms/step - loss: 1.2863 - accuracy: 0.3494 - val_loss: 1.1604 - val_accuracy: 0.3432\nEpoch 4/18\n36/36 [==============================] - 16s 443ms/step - loss: 1.2419 - accuracy: 0.3625 - val_loss: 1.1110 - val_accuracy: 0.3455\nEpoch 5/18\n36/36 [==============================] - 16s 445ms/step - loss: 1.1084 - accuracy: 0.4629 - val_loss: 1.0121 - val_accuracy: 0.4393\nEpoch 6/18\n36/36 [==============================] - 16s 444ms/step - loss: 0.8958 - accuracy: 0.6055 - val_loss: 0.8167 - val_accuracy: 0.6611\nEpoch 7/18\n36/36 [==============================] - 16s 444ms/step - loss: 1.0172 - accuracy: 0.5294 - val_loss: 0.8220 - val_accuracy: 0.6347\nEpoch 8/18\n36/36 [==============================] - 16s 444ms/step - loss: 0.8748 - accuracy: 0.6133 - val_loss: 0.7293 - val_accuracy: 0.7205\nEpoch 9/18\n36/36 [==============================] - 16s 441ms/step - loss: 0.7772 - accuracy: 0.6738 - val_loss: 0.6983 - val_accuracy: 0.7436\nEpoch 10/18\n36/36 [==============================] - 16s 440ms/step - loss: 0.7081 - accuracy: 0.7073 - val_loss: 0.7012 - val_accuracy: 0.7518\nEpoch 11/18\n36/36 [==============================] - 16s 444ms/step - loss: 0.6597 - accuracy: 0.7354 - val_loss: 0.6135 - val_accuracy: 0.7630\nEpoch 12/18\n36/36 [==============================] - 16s 444ms/step - loss: 0.6103 - accuracy: 0.7582 - val_loss: 0.5858 - val_accuracy: 0.7799\nEpoch 13/18\n36/36 [==============================] - 16s 443ms/step - loss: 0.5254 - accuracy: 0.7949 - val_loss: 0.6100 - val_accuracy: 0.7904\nEpoch 14/18\n36/36 [==============================] - 16s 442ms/step - loss: 0.4796 - accuracy: 0.8227 - val_loss: 0.5998 - val_accuracy: 0.7931\nEpoch 15/18\n36/36 [==============================] - 16s 443ms/step - loss: 0.4511 - accuracy: 0.8340 - val_loss: 0.6342 - val_accuracy: 0.7954\nEpoch 16/18\n36/36 [==============================] - 16s 444ms/step - loss: 0.4048 - accuracy: 0.8558 - val_loss: 0.5643 - val_accuracy: 0.8059\nEpoch 17/18\n36/36 [==============================] - 16s 446ms/step - loss: 0.3714 - accuracy: 0.8696 - val_loss: 0.6330 - val_accuracy: 0.8020\nEpoch 18/18\n36/36 [==============================] - 16s 444ms/step - loss: 0.3490 - accuracy: 0.8794 - val_loss: 0.6060 - val_accuracy: 0.7964\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7d89341d9ee0>"},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:40:26.052829Z","iopub.execute_input":"2023-07-10T04:40:26.053958Z","iopub.status.idle":"2023-07-10T04:40:26.115679Z","shell.execute_reply.started":"2023-07-10T04:40:26.053911Z","shell.execute_reply":"2023-07-10T04:40:26.114333Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"test_input = roberta_encode(test.premise.values, test.hypothesis.values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:40:28.501838Z","iopub.execute_input":"2023-07-10T04:40:28.502950Z","iopub.status.idle":"2023-07-10T04:40:31.373656Z","shell.execute_reply.started":"2023-07-10T04:40:28.502909Z","shell.execute_reply":"2023-07-10T04:40:31.372230Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:40:39.752672Z","iopub.execute_input":"2023-07-10T04:40:39.753631Z","iopub.status.idle":"2023-07-10T04:41:15.004653Z","shell.execute_reply.started":"2023-07-10T04:40:39.753590Z","shell.execute_reply":"2023-07-10T04:41:15.003314Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2023-07-10 04:40:49.516684: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-07-10 04:40:50.068823: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"163/163 [==============================] - 35s 88ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:41:17.938396Z","iopub.execute_input":"2023-07-10T04:41:17.938900Z","iopub.status.idle":"2023-07-10T04:41:17.961997Z","shell.execute_reply.started":"2023-07-10T04:41:17.938859Z","shell.execute_reply":"2023-07-10T04:41:17.960692Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:41:20.149035Z","iopub.execute_input":"2023-07-10T04:41:20.149511Z","iopub.status.idle":"2023-07-10T04:41:20.165428Z","shell.execute_reply.started":"2023-07-10T04:41:20.149478Z","shell.execute_reply":"2023-07-10T04:41:20.164210Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"           id  prediction\n0  c6d58c3f69           2\n1  cefcc82292           1\n2  e98005252c           0\n3  58518c10ba           1\n4  c32b0d16df           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c6d58c3f69</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cefcc82292</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e98005252c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58518c10ba</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c32b0d16df</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:41:24.169252Z","iopub.execute_input":"2023-07-10T04:41:24.169749Z","iopub.status.idle":"2023-07-10T04:41:24.187228Z","shell.execute_reply.started":"2023-07-10T04:41:24.169710Z","shell.execute_reply":"2023-07-10T04:41:24.186012Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(submission)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:51:42.196622Z","iopub.execute_input":"2023-07-10T04:51:42.197837Z","iopub.status.idle":"2023-07-10T04:51:42.207020Z","shell.execute_reply.started":"2023-07-10T04:51:42.197797Z","shell.execute_reply":"2023-07-10T04:51:42.205902Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"              id  prediction\n0     c6d58c3f69           2\n1     cefcc82292           1\n2     e98005252c           0\n3     58518c10ba           1\n4     c32b0d16df           1\n...          ...         ...\n5190  5f90dd59b0           0\n5191  f357a04e86           2\n5192  1f0ea92118           0\n5193  0407b48afb           0\n5194  16c2f2ab89           2\n\n[5195 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}